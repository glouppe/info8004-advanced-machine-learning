{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = \"\"\"Adabi et al, \"Deep learning with differential privacy\", 2016.\n",
    "Agarwal et al, \"Deep Reinforcement Learning at the Edge of the Statistical Precipice\", 2021.\n",
    "Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
    "Chen et al, \"Evaluating Large Language Models Trained on Code\", 2021.\n",
    "Dosovitskiy et al, \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", 2020.\n",
    "Gemp et al, \"EigenGame: PCA as a Nash Equilibrium\", 2021.\n",
    "Ilyas et al, \"Adversarial Examples Are Not Bugs, They Are Features\", 2019.\n",
    "Kilbertus et al, \"Avoiding discrimination through causal reasoning\", 2017.\n",
    "Koch et al, \"Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research\", 2021.\n",
    "Li et al, \"Competition-Level Code Generation with AlphaCode\", 2022.\n",
    "Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
    "Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
    "Niemyer and Geiger, \"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields\", 2021.\n",
    "Parisi et al, \"Continual lifelong learning with neural networks: a review\", 2019.\n",
    "Petar Veličković et al, \"Graph Attention Networks\", 2018.\n",
    "Petar Veličković et al, \"Neural Execution of Graph Algorithms\", 2022.\n",
    "Pfaff et al, \"Learning Mesh-Based Simulation with Graph Networks\", 2021.\n",
    "Radford et al, \"Learning Transferable Visual Models From Natural Language Supervision\", 2021.\n",
    "Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n",
    "Schölkopf et al., \"Towards causal representation learning\", 2021.\n",
    "Schrittwieser et al, \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\", 2019.\n",
    "Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
    "Song et al, \"Score-Based Generative Modeling through Stochastic Differential Equations\", 2021.\n",
    "Stewart and Ermon, \"Label-Free Supervision of Neural Networks with Physics and Domain Knowledge\", 2016.\n",
    "Tan et al, \"Efficientnet: Rethinking model scaling for convolutional neural networks\", 2019.\n",
    "Tolstikhin et al, \"MLP-Mixer: An all-MLP Architecture for Vision\", 2021.\n",
    "Udrescu and Tegmark, \"AI Feynman: A physics-inspired method for symbolic regression\", 2020.\n",
    "Yin et al, \"Augmenting physical models with deep networks for complex dynamics forecasting\", 2021.\"\"\"\n",
    "papers = papers.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = \"\"\"Yaman Altaha, Salah Amar, Capel Esteban Hernandez\n",
    "Maxime Amodei, Yassine Benzakour, Julien Schwanen\n",
    "Tristan Catteeuw, Brieuc Jamoulle\n",
    "Julien Carion, Meng Lin\n",
    "Dao Davan Chiem, Lorenzo Pagliarello, Laurent Stassain\n",
    "Julien Hubar, François Lievens, Corentin Van Putte\n",
    "Pierre Chapeau, Martin Peeters, Barry Sajid\n",
    "François Cubélier, Guilherme Ribeiro\n",
    "y Dominguez Lisa Bueres, Antoine Dekyvere, Delphine Remacle\n",
    "Clément Grodent, Corentin Merle, Guillaume Vrijens\n",
    "Poizat Adrien, Gaël Di Raimo\n",
    "Robin Perski, Antoine Verdonck\n",
    "Barry Sajid\n",
    "Petr Bakholdin, Aidar Galin, Aleksandr Samofalov\"\"\"\n",
    "groups = groups.split(\"\\n\")\n",
    "sum([len(g.split(\",\")) for g in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
      "--> Yaman Altaha, Salah Amar, Capel Esteban Hernandez\n",
      "\n",
      "- Petar Veličković et al, \"Neural Execution of Graph Algorithms\", 2022.\n",
      "--> Maxime Amodei, Yassine Benzakour, Julien Schwanen\n",
      "\n",
      "- Radford et al, \"Learning Transferable Visual Models From Natural Language Supervision\", 2021.\n",
      "--> Tristan Catteeuw, Brieuc Jamoulle\n",
      "\n",
      "- Agarwal et al, \"Deep Reinforcement Learning at the Edge of the Statistical Precipice\", 2021.\n",
      "--> Julien Carion, Meng Lin\n",
      "\n",
      "- Kilbertus et al, \"Avoiding discrimination through causal reasoning\", 2017.\n",
      "--> Dao Davan Chiem, Lorenzo Pagliarello, Laurent Stassain\n",
      "\n",
      "- Stewart and Ermon, \"Label-Free Supervision of Neural Networks with Physics and Domain Knowledge\", 2016.\n",
      "--> Julien Hubar, François Lievens, Corentin Van Putte\n",
      "\n",
      "- Koch et al, \"Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research\", 2021.\n",
      "--> Pierre Chapeau, Martin Peeters, Barry Sajid\n",
      "\n",
      "- Tan et al, \"Efficientnet: Rethinking model scaling for convolutional neural networks\", 2019.\n",
      "--> François Cubélier, Guilherme Ribeiro\n",
      "\n",
      "- Schölkopf et al., \"Towards causal representation learning\", 2021.\n",
      "--> y Dominguez Lisa Bueres, Antoine Dekyvere, Delphine Remacle\n",
      "\n",
      "- Adabi et al, \"Deep learning with differential privacy\", 2016.\n",
      "--> Clément Grodent, Corentin Merle, Guillaume Vrijens\n",
      "\n",
      "- Yin et al, \"Augmenting physical models with deep networks for complex dynamics forecasting\", 2021.\n",
      "--> Poizat Adrien, Gaël Di Raimo\n",
      "\n",
      "- Dosovitskiy et al, \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", 2020.\n",
      "--> Robin Perski, Antoine Verdonck\n",
      "\n",
      "- Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
      "--> Barry Sajid\n",
      "\n",
      "- Li et al, \"Competition-Level Code Generation with AlphaCode\", 2022.\n",
      "--> Petr Bakholdin, Aidar Galin, Aleksandr Samofalov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "\n",
    "permutation = np.random.permutation(len(papers))\n",
    "for g, t in zip(groups, permutation):\n",
    "    print(\"-\", papers[t])\n",
    "    print(\"-->\", g) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tolstikhin et al, \"MLP-Mixer: An all-MLP Architecture for Vision\", 2021.\n",
      "- Parisi et al, \"Continual lifelong learning with neural networks: a review\", 2019.\n",
      "- Gemp et al, \"EigenGame: PCA as a Nash Equilibrium\", 2021.\n",
      "- Petar Veličković et al, \"Graph Attention Networks\", 2018.\n",
      "- Song et al, \"Score-Based Generative Modeling through Stochastic Differential Equations\", 2021.\n",
      "- Chen et al, \"Evaluating Large Language Models Trained on Code\", 2021.\n",
      "- Niemyer and Geiger, \"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields\", 2021.\n",
      "- Schrittwieser et al, \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\", 2019.\n",
      "- Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
      "- Ilyas et al, \"Adversarial Examples Are Not Bugs, They Are Features\", 2019.\n",
      "- Udrescu and Tegmark, \"AI Feynman: A physics-inspired method for symbolic regression\", 2020.\n",
      "- Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
      "- Pfaff et al, \"Learning Mesh-Based Simulation with Graph Networks\", 2021.\n",
      "- Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n"
     ]
    }
   ],
   "source": [
    "# Unassigned papers\n",
    "for t in permutation[len(groups):]:\n",
    "    print(\"-\", papers[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swaps\n",
    "\n",
    "- Dao Davan Chiem, Lorenzo Pagliarello, Laurent Stassain change `Kilbertus et al, \"Avoiding discrimination through causal reasoning\", 2017` for `Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019`.\n",
    "- Julien Carion, Meng Lin change `Agarwal et al, \"Deep Reinforcement Learning at the Edge of the Statistical Precipice\", 2021` for `Schrittwieser et al, \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\", 2019`.\n",
    "- Barry Sajid joins the group of Pierre Chapeau and Martin Peeters. The paper `Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019` is now available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
