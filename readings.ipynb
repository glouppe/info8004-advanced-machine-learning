{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student presentations\n",
    "\n",
    "Instructions:\n",
    "- You are tasked to deliver a 20-minute lecture on the paper and its necessary background, followed by a 10-minute Q&A session.\n",
    "- Your slides will have to be submitted on Gradescope the day before your presentation.\n",
    "- Presentations are public. You are all requested to attend all presentations and support your peers.\n",
    "- This assignment will count for 40% of the final grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = \"\"\"Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
    "Chowdhery, \"PaLM: Scaling Language Modeling with Pathways\", 2022.\n",
    "Kaplan et al, \"Scaling Laws for Neural Language Models\", 2022.\n",
    "Koch et al, \"Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research\", 2021.\n",
    "Krizhevsky et al, \"ImageNet Classification with Deep Convolutional Neural Networks\", 2012.\n",
    "Li et al, \"Competition-Level Code Generation with AlphaCode\", 2022.\n",
    "Liu et al, \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\", 2021.\n",
    "Lofti et al, \"Bayesian Model Selection, the Marginal Likelihood, and Generalization\", 2022.\n",
    "Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
    "Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
    "Ouyang et al, \"Training language models to follow instructions with human feedback\", 2022.\n",
    "Pfaff et al, \"Learning Mesh-Based Simulation with Graph Networks\", 2021.\n",
    "Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n",
    "Saharia et al, \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", 2022\n",
    "Schuhmann et al, \"LAION-5B: An open large-scale dataset for training next generation image-text models\", 2022.\n",
    "Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
    "Song et al, \"Score-Based Generative Modeling through Stochastic Differential Equations\", 2021.\n",
    "Tolstikhin et al, \"MLP-Mixer: An all-MLP Architecture for Vision\", 2021.\n",
    "Ravuri et al, \"Skilful precipitation nowcasting using deep generative models of radar\", 2021.\n",
    "Chen et al, \"Neural ordinary differential equations\", 2018\n",
    "Grinsztajn et al., \"Why do tree-based models still outperform deep learning on typical tabular data?\", 2022\n",
    "Sohn et al., \"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\", 2020\"\"\"\n",
    "papers = papers.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = \"\"\"Arthur Louette, Sacha Lewin\n",
    "Laurie Boveroux, Marco Naa\n",
    "Ahmed Alakhir, Justin Henrotte\n",
    "Lucie Navez, Axelle Schyns\n",
    "Joachim Houyon, Brice Baguette\n",
    "Maziane Yassine, Ural Seyfullah\n",
    "Boris Courtoy\n",
    "Robin Coutelier\n",
    "Lin Meng\"\"\"\n",
    "groups = groups.split(\"\\n\")\n",
    "sum([len(g.split(\",\")) for g in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Breiman, \"Statistical modeling: the Two Cultures\", 2001.\n",
      "--> Arthur Louette, Sacha Lewin\n",
      "\n",
      "- Saharia et al, \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\", 2022\n",
      "--> Laurie Boveroux, Marco Naa\n",
      "\n",
      "- Nakkiran et al., \"Deep Double Descent: Where Bigger Models and More Data Hurt\", 2019\n",
      "--> Ahmed Alakhir, Justin Henrotte\n",
      "\n",
      "- Chowdhery, \"PaLM: Scaling Language Modeling with Pathways\", 2022.\n",
      "--> Lucie Navez, Axelle Schyns\n",
      "\n",
      "- Sitzmann et al, \"Implicit Neural Representations with Periodic Activation Functions\", 2020.\n",
      "--> Joachim Houyon, Brice Baguette\n",
      "\n",
      "- Li et al, \"Competition-Level Code Generation with AlphaCode\", 2022.\n",
      "--> Maziane Yassine, Ural Seyfullah\n",
      "\n",
      "- Grinsztajn et al., \"Why do tree-based models still outperform deep learning on typical tabular data?\", 2022\n",
      "--> Boris Courtoy\n",
      "\n",
      "- Pfaff et al, \"Learning Mesh-Based Simulation with Graph Networks\", 2021.\n",
      "--> Robin Coutelier\n",
      "\n",
      "- Koch et al, \"Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research\", 2021.\n",
      "--> Lin Meng\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "permutation = np.random.permutation(len(papers))\n",
    "for g, t in zip(groups, permutation):\n",
    "    print(\"-\", papers[t])\n",
    "    print(\"-->\", g) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Krizhevsky et al, \"ImageNet Classification with Deep Convolutional Neural Networks\", 2012.\n",
      "- Tolstikhin et al, \"MLP-Mixer: An all-MLP Architecture for Vision\", 2021.\n",
      "- Rudin, \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\", 2019.\n",
      "- Ravuri et al, \"Skilful precipitation nowcasting using deep generative models of radar\", 2021.\n",
      "- Song et al, \"Score-Based Generative Modeling through Stochastic Differential Equations\", 2021.\n",
      "- Kaplan et al, \"Scaling Laws for Neural Language Models\", 2022.\n",
      "- Neyshabur, \"The role of over-parametrization in generalization of neural networks\", 2019.\n",
      "- Sohn et al., \"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\", 2020\n",
      "- Lofti et al, \"Bayesian Model Selection, the Marginal Likelihood, and Generalization\", 2022.\n",
      "- Ouyang et al, \"Training language models to follow instructions with human feedback\", 2022.\n",
      "- Schuhmann et al, \"LAION-5B: An open large-scale dataset for training next generation image-text models\", 2022.\n",
      "- Chen et al, \"Neural ordinary differential equations\", 2018\n",
      "- Liu et al, \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\", 2021.\n"
     ]
    }
   ],
   "source": [
    "# Unassigned papers\n",
    "for t in permutation[len(groups):]:\n",
    "    print(\"-\", papers[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swaps\n",
    "\n",
    "You are allowed to change your paper for any of the unassigned papers above. Remaining papers will be assigned on a first-come-first-served basis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
